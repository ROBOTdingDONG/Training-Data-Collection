{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ROBOTdingDONG/Training-Data-Collection/blob/main/Snake_AI_Simulation_(Q_Learning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import collections\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import sys\n",
        "from collections import namedtuple\n",
        "\n",
        "# --- Logging Setup ---\n",
        "# Configure logging to save to a file and print to console\n",
        "log_filename = f\"snake_rl_log_{time.strftime('%Y%m%d_%H%M%S')}.log\"\n",
        "logging.basicConfig(level=logging.INFO, # Set level to INFO (or DEBUG for more detail)\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "                    handlers=[\n",
        "                        logging.FileHandler(log_filename), # Log to file\n",
        "                        logging.StreamHandler(sys.stdout) # Log to console\n",
        "                    ])\n",
        "logging.info(\"--- Snake RL Simulation Initialized ---\")\n",
        "\n",
        "# --- Game Components ---\n",
        "Point = namedtuple('Point', ['x', 'y']) # Simple coordinate representation\n",
        "\n",
        "# Absolute directions (indices map to vectors)\n",
        "DIRECTIONS = {\n",
        "    0: Point(0, -1),  # Up\n",
        "    1: Point(0, 1),   # Down\n",
        "    2: Point(-1, 0),  # Left\n",
        "    3: Point(1, 0)    # Right\n",
        "}\n",
        "# Relative actions for the agent (relative to its current direction)\n",
        "# 0: Go Straight, 1: Turn Left, 2: Turn Right\n",
        "RELATIVE_ACTIONS = [0, 1, 2]\n",
        "\n",
        "# --- Game Environment ---\n",
        "class SnakeGame:\n",
        "    \"\"\"\n",
        "    Represents the Snake game environment.\n",
        "    Handles game state, rules, actions, and rewards.\n",
        "    \"\"\"\n",
        "    def __init__(self, width=10, height=10):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.reset()\n",
        "        logging.info(f\"Game environment created ({width}x{height} grid).\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the game to the starting state for a new episode.\"\"\"\n",
        "        # Place snake in the middle, moving right initially\n",
        "        self.head = Point(self.width // 2, self.height // 2)\n",
        "        self.snake = [self.head,\n",
        "                      Point(self.head.x - 1, self.head.y),\n",
        "                      Point(self.head.x - 2, self.head.y)]\n",
        "        self.direction = 3 # Start moving right (index in DIRECTIONS)\n",
        "        self.score = 0\n",
        "        self.food = None\n",
        "        self._place_food() # Place initial food\n",
        "        self.game_over = False\n",
        "        self.steps_taken = 0\n",
        "        self.steps_since_food = 0\n",
        "        # Heuristic limit to prevent infinite loops if agent gets stuck\n",
        "        self.max_steps_no_food = self.width * self.height * 2\n",
        "        # Return the initial state representation for the agent\n",
        "        return self._get_state()\n",
        "\n",
        "    def _place_food(self):\n",
        "        \"\"\"Places food randomly on the grid, ensuring it's not on the snake.\"\"\"\n",
        "        while True:\n",
        "            x = random.randint(0, self.width - 1)\n",
        "            y = random.randint(0, self.height - 1)\n",
        "            self.food = Point(x, y)\n",
        "            # Ensure food is not placed where the snake currently is\n",
        "            if self.food not in self.snake:\n",
        "                break\n",
        "\n",
        "    def _move(self, action_relative):\n",
        "        \"\"\"\n",
        "        Updates the snake's position based on a relative action.\n",
        "        action_relative: 0 (Straight), 1 (Left Turn), 2 (Right Turn)\n",
        "        \"\"\"\n",
        "        # Determine the new absolute direction based on the relative action\n",
        "        if action_relative == 1: # Turn Left\n",
        "            new_direction_idx = (self.direction - 1 + 4) % 4 # +4 ensures positive result\n",
        "        elif action_relative == 2: # Turn Right\n",
        "            new_direction_idx = (self.direction + 1) % 4\n",
        "        else: # Go Straight (action_relative == 0)\n",
        "            new_direction_idx = self.direction\n",
        "\n",
        "        self.direction = new_direction_idx # Update snake's current direction\n",
        "        move_vector = DIRECTIONS[self.direction] # Get the (dx, dy) vector for the move\n",
        "\n",
        "        # Calculate new head position\n",
        "        self.head = Point(self.head.x + move_vector.x, self.head.y + move_vector.y)\n",
        "        # Insert new head at the beginning of the snake list\n",
        "        self.snake.insert(0, self.head)\n",
        "\n",
        "    def _is_collision(self, pt=None):\n",
        "        \"\"\"\n",
        "        Checks if a given point (or the snake's head by default) results in a collision.\n",
        "        Collisions occur with walls or the snake's own body.\n",
        "        \"\"\"\n",
        "        if pt is None:\n",
        "            pt = self.head # Check collision for the current head position if no point provided\n",
        "\n",
        "        # Check wall collision\n",
        "        if not (0 <= pt.x < self.width and 0 <= pt.y < self.height):\n",
        "            return True\n",
        "        # Check self collision (if the point is anywhere in the snake's body, excluding the head itself)\n",
        "        if pt in self.snake[1:]:\n",
        "            return True\n",
        "        # No collision\n",
        "        return False\n",
        "\n",
        "    def step(self, action_relative):\n",
        "        \"\"\"\n",
        "        Performs one step in the game based on the agent's action.\n",
        "        Args:\n",
        "            action_relative: The relative action chosen by the agent (0, 1, or 2).\n",
        "        Returns:\n",
        "            tuple: (next_state, reward, game_over)\n",
        "                   - next_state: The state representation after the action.\n",
        "                   - reward: The numerical reward obtained from this step.\n",
        "                   - game_over: Boolean indicating if the game ended.\n",
        "        \"\"\"\n",
        "        self.steps_taken += 1\n",
        "        self.steps_since_food += 1\n",
        "\n",
        "        # Move the snake according to the action\n",
        "        self._move(action_relative)\n",
        "\n",
        "        # Define rewards/penalties\n",
        "        reward = 0 # Default reward per step\n",
        "\n",
        "        # Check for game over conditions (collision or starvation)\n",
        "        if self._is_collision() or self.steps_since_food > self.max_steps_no_food:\n",
        "            self.game_over = True\n",
        "            reward = -100 # Significant penalty for dying/starving\n",
        "            logging.debug(f\"Game Over. Collision: {self._is_collision()}, Starvation: {self.steps_since_food > self.max_steps_no_food}\")\n",
        "            # Return immediately as the game ended\n",
        "            return self._get_state(), reward, self.game_over\n",
        "\n",
        "        # Check if the snake ate the food\n",
        "        if self.head == self.food:\n",
        "            self.score += 1\n",
        "            reward = 20 # Positive reward for eating food\n",
        "            self._place_food() # Place new food\n",
        "            self.steps_since_food = 0 # Reset starvation counter\n",
        "            # Snake grows, so we don't pop the tail\n",
        "            logging.debug(f\"Food eaten! Score: {self.score}\")\n",
        "        else:\n",
        "            # Snake moves, so remove the last segment (tail) if no food was eaten\n",
        "            self.snake.pop()\n",
        "            # Small penalty per step to encourage finding food faster\n",
        "            reward = -0.1\n",
        "\n",
        "        # Return the outcome of the step\n",
        "        return self._get_state(), reward, self.game_over\n",
        "\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"\n",
        "        Generates a state representation for the Q-learning agent.\n",
        "        This converts the complex game situation into a simplified, hashable tuple\n",
        "        that the agent can use to look up Q-values.\n",
        "\n",
        "        State tuple components:\n",
        "        1. Danger Straight: Is there an immediate collision if moving straight? (0=No, 1=Yes)\n",
        "        2. Danger Left: Is there an immediate collision if turning left? (0=No, 1=Yes)\n",
        "        3. Danger Right: Is there an immediate collision if turning right? (0=No, 1=Yes)\n",
        "        4. Food Direction X: Relative X direction of food (-1=Left, 0=Same Col, 1=Right)\n",
        "        5. Food Direction Y: Relative Y direction of food (-1=Up, 0=Same Row, 1=Down)\n",
        "        \"\"\"\n",
        "        head = self.snake[0]\n",
        "\n",
        "        # Determine the points immediately straight, left, and right relative to the snake's current direction\n",
        "        point_straight, point_left, point_right = self._get_relative_points()\n",
        "\n",
        "        # Check for collision danger at these relative points\n",
        "        danger_straight = self._is_collision(point_straight)\n",
        "        danger_left = self._is_collision(point_left)\n",
        "        danger_right = self._is_collision(point_right)\n",
        "\n",
        "        # Determine the general direction of the food relative to the snake's head\n",
        "        food_dir_x = 0\n",
        "        if self.food.x < head.x: food_dir_x = -1 # Food is to the Left\n",
        "        elif self.food.x > head.x: food_dir_x = 1 # Food is to the Right\n",
        "\n",
        "        food_dir_y = 0\n",
        "        if self.food.y < head.y: food_dir_y = -1 # Food is Up\n",
        "        elif self.food.y > head.y: food_dir_y = 1 # Food is Down\n",
        "\n",
        "        # Compile the state information into a tuple (must be hashable for dictionary keys)\n",
        "        state = (\n",
        "            int(danger_straight), int(danger_left), int(danger_right),\n",
        "            food_dir_x, food_dir_y,\n",
        "            # Optional: Could add current direction, but increases state space size\n",
        "            # self.direction\n",
        "        )\n",
        "        return state\n",
        "\n",
        "    def _get_relative_points(self):\n",
        "        \"\"\"Calculates the coordinates of the points directly straight, left, and right\n",
        "           relative to the snake's current heading.\"\"\"\n",
        "        head = self.snake[0]\n",
        "        current_dir_vector = DIRECTIONS[self.direction] # Vector for current direction (dx, dy)\n",
        "\n",
        "        # Calculate vectors for relative left and right turns using vector rotation\n",
        "        # Left Turn: If current=(dx, dy), left=(-dy, dx)\n",
        "        left_dir_vector = Point(-current_dir_vector.y, current_dir_vector.x)\n",
        "        # Right Turn: If current=(dx, dy), right=(dy, -dx)\n",
        "        right_dir_vector = Point(current_dir_vector.y, -current_dir_vector.x)\n",
        "\n",
        "        # Calculate the actual points on the grid\n",
        "        point_straight = Point(head.x + current_dir_vector.x, head.y + current_dir_vector.y)\n",
        "        point_left = Point(head.x + left_dir_vector.x, head.y + left_dir_vector.y)\n",
        "        point_right = Point(head.x + right_dir_vector.x, head.y + right_dir_vector.y)\n",
        "\n",
        "        return point_straight, point_left, point_right\n",
        "\n",
        "    def render_text(self):\n",
        "        \"\"\"(Optional) Renders a simple text-based view of the game state to the console.\"\"\"\n",
        "        # Create an empty grid representation\n",
        "        grid = [['.' for _ in range(self.width)] for _ in range(self.height)]\n",
        "        # Place food\n",
        "        if self.food:\n",
        "            if 0 <= self.food.y < self.height and 0 <= self.food.x < self.width:\n",
        "                 grid[self.food.y][self.food.x] = 'F'\n",
        "        # Place snake (Head 'H', Body 'S')\n",
        "        for i, segment in enumerate(self.snake):\n",
        "            if 0 <= segment.y < self.height and 0 <= segment.x < self.width:\n",
        "                grid[segment.y][segment.x] = 'H' if i == 0 else 'S'\n",
        "\n",
        "        # Print grid row by row\n",
        "        print(\"-\" * (self.width * 2 + 1)) # Top border\n",
        "        for row in grid:\n",
        "            print(\"|\" + \" \".join(row) + \"|\") # Rows with borders\n",
        "        print(\"-\" * (self.width * 2 + 1)) # Bottom border\n",
        "        print(f\"Score: {self.score} | Steps: {self.steps_taken} | Steps w/o Food: {self.steps_since_food}\")\n",
        "\n",
        "\n",
        "# --- Q-Learning Agent ---\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    Implements the Q-learning algorithm to learn how to play Snake.\n",
        "    Manages the Q-table, exploration/exploitation strategy, and learning updates.\n",
        "    \"\"\"\n",
        "    def __init__(self, actions, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, exploration_decay=0.9995, min_exploration_rate=0.01):\n",
        "        self.actions = actions # List of possible actions (e.g., [0, 1, 2])\n",
        "        self.alpha = learning_rate      # Learning rate (how much new info overrides old)\n",
        "        self.gamma = discount_factor    # Discount factor (importance of future rewards)\n",
        "        self.epsilon = exploration_rate # Initial exploration probability\n",
        "        self.epsilon_decay = exploration_decay # Rate at which exploration decreases\n",
        "        self.min_epsilon = min_exploration_rate # Minimum exploration probability\n",
        "        # Q-table: Stores Q-values for (state, action) pairs.\n",
        "        # Uses nested defaultdicts for easy handling of unseen states/actions.\n",
        "        # Q[state] -> {action1: value1, action2: value2, ...}\n",
        "        self.q_table = collections.defaultdict(lambda: collections.defaultdict(float))\n",
        "        self.training_mode = True # Agent learns when True\n",
        "        logging.info(\"Q-Learning Agent created.\")\n",
        "        logging.info(f\"  Learning Rate (alpha): {self.alpha}\")\n",
        "        logging.info(f\"  Discount Factor (gamma): {self.gamma}\")\n",
        "        logging.info(f\"  Initial Epsilon: {self.epsilon}\")\n",
        "        logging.info(f\"  Epsilon Decay: {self.epsilon_decay}\")\n",
        "        logging.info(f\"  Min Epsilon: {self.min_epsilon}\")\n",
        "\n",
        "    def set_training_mode(self, mode=True):\n",
        "        \"\"\"Enable or disable learning and exploration.\"\"\"\n",
        "        self.training_mode = mode\n",
        "        if not mode:\n",
        "            self.epsilon = 0 # No exploration if not training (pure exploitation)\n",
        "        logging.info(f\"Agent training mode set to: {self.training_mode}\")\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Selects an action based on the current state using the epsilon-greedy strategy.\n",
        "        - With probability epsilon: Choose a random action (explore).\n",
        "        - Otherwise: Choose the action with the highest Q-value for the state (exploit).\n",
        "        \"\"\"\n",
        "        if self.training_mode and random.uniform(0, 1) < self.epsilon:\n",
        "            # --- Exploration ---\n",
        "            action = random.choice(self.actions)\n",
        "            logging.debug(f\"State: {state} -> Explore Action: {action}\")\n",
        "        else:\n",
        "            # --- Exploitation ---\n",
        "            # Get the Q-values for all possible actions from the current state\n",
        "            q_values_for_state = self.q_table[state]\n",
        "\n",
        "            # If this state has never been seen before, Q-values are default (0.0).\n",
        "            # In this case, choose a random action as there's no learned preference.\n",
        "            if not q_values_for_state:\n",
        "                 action = random.choice(self.actions)\n",
        "                 logging.debug(f\"State: {state} (unseen) -> Random Action (Exploit fallback): {action}\")\n",
        "            else:\n",
        "                # Find the maximum Q-value among the actions for this state\n",
        "                max_q_value = max(q_values_for_state.values())\n",
        "                # Get all actions that have this maximum Q-value (could be ties)\n",
        "                best_actions = [a for a, q in q_values_for_state.items() if q == max_q_value]\n",
        "                # Choose randomly among the best actions to break ties\n",
        "                action = random.choice(best_actions)\n",
        "                logging.debug(f\"State: {state} -> Exploit Action: {action} (MaxQ={max_q_value:.3f})\")\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Updates the Q-value for the executed state-action pair using the Q-learning rule.\n",
        "        This is where the agent \"learns\" from its experience.\n",
        "\n",
        "        Args:\n",
        "            state: The state before the action was taken.\n",
        "            action: The action taken by the agent.\n",
        "            reward: The reward received after taking the action.\n",
        "            next_state: The state the agent transitioned to after the action.\n",
        "            done: Boolean indicating if the episode ended after this action.\n",
        "        \"\"\"\n",
        "        if not self.training_mode:\n",
        "            return # Do not update Q-table if not in training mode\n",
        "\n",
        "        # --- Q-Learning Update Rule ---\n",
        "        # Q(s, a) <- Q(s, a) + alpha * [Target - Q(s, a)]\n",
        "        # where Target = reward + gamma * max_a'(Q(s', a'))  (if not done)\n",
        "        #       Target = reward                           (if done)\n",
        "        # s: current state, a: current action\n",
        "        # s': next state, a': possible actions in next state\n",
        "        # alpha: learning rate, gamma: discount factor\n",
        "\n",
        "        # 1. Get the current Q-value for the (state, action) pair. Default is 0.0 if not seen.\n",
        "        current_q = self.q_table[state][action]\n",
        "\n",
        "        # 2. Calculate the 'Target' value (the estimate of the optimal future value)\n",
        "        if done:\n",
        "            # If the episode is over, there's no next state to consider.\n",
        "            # The target is simply the final reward received.\n",
        "            target = reward\n",
        "            logging.debug(f\"Learn (End State): Target = Reward = {reward:.2f}\")\n",
        "        else:\n",
        "            # If the episode is not over, estimate the value of the next state.\n",
        "            # Find the maximum Q-value among all possible actions in the 'next_state'.\n",
        "            next_q_values = self.q_table[next_state] # Get {action: Q-value} dict for next state\n",
        "            max_next_q = max(next_q_values.values()) if next_q_values else 0.0 # Max Q-value, or 0 if next_state is new\n",
        "            # The target includes the immediate reward plus the discounted estimated value of the future (max_next_q).\n",
        "            target = reward + self.gamma * max_next_q\n",
        "            logging.debug(f\"Learn: R={reward:.1f}, gamma={self.gamma}, max_next_Q={max_next_q:.3f} -> Target={target:.3f}\")\n",
        "\n",
        "        # 3. Calculate the update amount (TD Error scaled by learning rate)\n",
        "        update_amount = self.alpha * (target - current_q)\n",
        "\n",
        "        # 4. Update the Q-value in the table\n",
        "        new_q = current_q + update_amount\n",
        "        self.q_table[state][action] = new_q\n",
        "\n",
        "        logging.debug(f\"  State={state}, Action={action}\")\n",
        "        logging.debug(f\"  CurrentQ={current_q:.3f}, Target={target:.3f}, Update={update_amount:.3f} -> NewQ={new_q:.3f}\")\n",
        "\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decreases the exploration rate (epsilon) over time, down to a minimum value.\"\"\"\n",
        "        if self.training_mode:\n",
        "            old_epsilon = self.epsilon\n",
        "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "            if self.epsilon != old_epsilon:\n",
        "                 logging.debug(f\"Epsilon decayed from {old_epsilon:.4f} to {self.epsilon:.4f}\")\n",
        "\n",
        "    def save_q_table(self, filename=\"q_table.json\"):\n",
        "        \"\"\"Saves the learned Q-table to a JSON file.\"\"\"\n",
        "        # Convert tuple keys (states) to strings for JSON compatibility\n",
        "        serializable_q_table = {str(k): dict(v) for k, v in self.q_table.items()}\n",
        "        try:\n",
        "            with open(filename, 'w') as f:\n",
        "                json.dump(serializable_q_table, f, indent=4)\n",
        "            logging.info(f\"Q-table (size {len(self.q_table)}) saved to {filename}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving Q-table to {filename}: {e}\")\n",
        "\n",
        "    def load_q_table(self, filename=\"q_table.json\"):\n",
        "        \"\"\"Loads a Q-table from a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                loaded_q_table_str_keys = json.load(f)\n",
        "\n",
        "            # Convert string keys back to tuples and inner dict keys/values appropriately\n",
        "            self.q_table = collections.defaultdict(lambda: collections.defaultdict(float))\n",
        "            loaded_count = 0\n",
        "            for state_str, actions_dict in loaded_q_table_str_keys.items():\n",
        "                try:\n",
        "                    # Convert state string '(int, int, ...)' back to tuple of ints\n",
        "                    # This assumes the state tuple contains only integers\n",
        "                    state_tuple = tuple(map(int, state_str.strip('()').split(',')))\n",
        "                except ValueError:\n",
        "                    logging.warning(f\"Skipping invalid state key format during load: {state_str}\")\n",
        "                    continue # Skip this state if parsing fails\n",
        "\n",
        "                # Convert inner action keys (strings) to ints and values to floats\n",
        "                inner_dict = collections.defaultdict(float)\n",
        "                for action_str, q_value in actions_dict.items():\n",
        "                    try:\n",
        "                         inner_dict[int(action_str)] = float(q_value)\n",
        "                    except ValueError:\n",
        "                         logging.warning(f\"Skipping invalid action/q-value format in state {state_str}: action={action_str}, value={q_value}\")\n",
        "                         continue # Skip invalid action entry\n",
        "\n",
        "                self.q_table[state_tuple] = inner_dict\n",
        "                loaded_count += 1\n",
        "\n",
        "            logging.info(f\"Q-table loaded from {filename}. {loaded_count} states loaded. Total size: {len(self.q_table)} states.\")\n",
        "        except FileNotFoundError:\n",
        "            logging.warning(f\"Q-table file '{filename}' not found. Starting with an empty table.\")\n",
        "        except json.JSONDecodeError:\n",
        "             logging.error(f\"Error decoding JSON from Q-table file '{filename}'. Starting with empty table.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading Q-table from {filename}: {e}. Starting with an empty table.\")\n",
        "\n",
        "\n",
        "# --- Simulation Loop ---\n",
        "def run_simulation(episodes=10000, render_every_n=0, log_every_n=100, save_q_table_every_n=5000, load_filename=None):\n",
        "    \"\"\"\n",
        "    Runs the main simulation loop for training the agent.\n",
        "\n",
        "    Args:\n",
        "        episodes: Total number of games (episodes) to simulate.\n",
        "        render_every_n: Render the game board every N episodes (0 to disable).\n",
        "        log_every_n: Log summary statistics every N episodes.\n",
        "        save_q_table_every_n: Save the Q-table every N episodes (0 to disable).\n",
        "        load_filename: Path to a Q-table file to load before starting, or None.\n",
        "    \"\"\"\n",
        "    game = SnakeGame(width=10, height=10) # Create the game instance\n",
        "    agent = QLearningAgent(actions=RELATIVE_ACTIONS) # Create the agent instance\n",
        "\n",
        "    # Load pre-trained Q-table if specified\n",
        "    if load_filename:\n",
        "        agent.load_q_table(load_filename)\n",
        "\n",
        "    # Lists to store results for analysis/logging\n",
        "    episode_scores = []\n",
        "    episode_steps = []\n",
        "    max_score_so_far = -1\n",
        "\n",
        "    logging.info(f\"--- Starting Simulation: {episodes} episodes ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "        state = game.reset() # Start a new game\n",
        "        done = False\n",
        "        current_episode_steps = 0\n",
        "\n",
        "        # --- Single Episode Loop ---\n",
        "        while not done:\n",
        "            # 1. Agent chooses action based on current state\n",
        "            action = agent.choose_action(state)\n",
        "\n",
        "            # 2. Environment executes action and returns outcome\n",
        "            next_state, reward, done = game.step(action)\n",
        "\n",
        "            # 3. Agent learns from the transition (state, action, reward, next_state, done)\n",
        "            agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "            # 4. Update current state for the next iteration\n",
        "            state = next_state\n",
        "            current_episode_steps += 1\n",
        "\n",
        "            # Optional: Render the game board\n",
        "            if render_every_n > 0 and episode % render_every_n == 0:\n",
        "                 # Simple console clearing (may vary by OS)\n",
        "                 # print(\"\\033[H\\033[J\", end=\"\") # Clears console on Linux/macOS\n",
        "                 game.render_text()\n",
        "                 print(f\"Episode: {episode}, Step: {current_episode_steps}, Action: {action}, Reward: {reward:.1f}\")\n",
        "                 print(f\"State: {state}\")\n",
        "                 time.sleep(0.05) # Pause briefly to make rendering viewable\n",
        "\n",
        "        # --- End of Episode ---\n",
        "        agent.decay_epsilon() # Decrease exploration chance for next episode\n",
        "        episode_scores.append(game.score)\n",
        "        episode_steps.append(current_episode_steps)\n",
        "        if game.score > max_score_so_far:\n",
        "            max_score_so_far = game.score\n",
        "\n",
        "        # Log summary statistics periodically\n",
        "        if episode % log_every_n == 0:\n",
        "             avg_score = sum(episode_scores[-log_every_n:]) / log_every_n\n",
        "             avg_steps = sum(episode_steps[-log_every_n:]) / log_every_n\n",
        "             logging.info(f\"Ep {episode}/{episodes} | \"\n",
        "                          f\"Last Score: {game.score} | Max Score: {max_score_so_far} | \"\n",
        "                          f\"Avg Score ({log_every_n} ep): {avg_score:.2f} | \"\n",
        "                          f\"Avg Steps ({log_every_n} ep): {avg_steps:.1f} | \"\n",
        "                          f\"Epsilon: {agent.epsilon:.4f} | \"\n",
        "                          f\"Q-States: {len(agent.q_table)}\")\n",
        "\n",
        "        # Save Q-table periodically\n",
        "        if save_q_table_every_n > 0 and episode % save_q_table_every_n == 0:\n",
        "            agent.save_q_table(f\"q_table_ep_{episode}.json\")\n",
        "\n",
        "    # --- End of Simulation ---\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    logging.info(f\"--- Simulation Finished ---\")\n",
        "    logging.info(f\"Total Episodes: {episodes}\")\n",
        "    logging.info(f\"Total Time: {total_time:.2f} seconds ({total_time/episodes:.4f} sec/ep)\")\n",
        "    logging.info(f\"Final Max Score: {max_score_so_far}\")\n",
        "    logging.info(f\"Final Q-Table size: {len(agent.q_table)} states.\")\n",
        "\n",
        "    # Save the final Q-table\n",
        "    agent.save_q_table(\"q_table_final.json\")\n",
        "\n",
        "    # Potential next step: Add plotting of scores over episodes using matplotlib\n",
        "\n",
        "\n",
        "# --- Main Execution Guard ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Configure simulation parameters here\n",
        "    NUM_EPISODES = 20000       # How many games to play for training\n",
        "    RENDER_EVERY = 0         # Show game board every N episodes (0 = never)\n",
        "    LOG_EVERY = 100          # Print summary log every N episodes\n",
        "    SAVE_EVERY = 5000        # Save Q-table snapshot every N episodes\n",
        "    LOAD_FILE = None         # Set to \"q_table_final.json\" or similar to resume training\n",
        "\n",
        "    run_simulation(\n",
        "        episodes=NUM_EPISODES,\n",
        "        render_every_n=RENDER_EVERY,\n",
        "        log_every_n=LOG_EVERY,\n",
        "        save_q_table_every_n=SAVE_EVERY,\n",
        "        load_filename=LOAD_FILE\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "U3A97ieyN4rW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEfr2U8bOT7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}